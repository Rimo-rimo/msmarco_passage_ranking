{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from pymilvus.model.sparse.bm25.tokenizers import build_default_analyzer\n",
    "from pymilvus.model.sparse import BM25EmbeddingFunction\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from pymilvus import model\n",
    "from pymilvus import MilvusClient, Collection, connections, DataType, CollectionSchema, FieldSchema\n",
    "import numpy as np\n",
    "import json\n",
    "from FlagEmbedding import FlagReranker\n",
    "from pymilvus.model.reranker import BGERerankFunction\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BM25 모델 Load\n",
    "def get_bm25_model(model_path:str = \"./files/bm25_msmarco_v1.json\",\n",
    "                   analyzer_language:str = \"en\"):\n",
    "    analyzer = build_default_analyzer(language=analyzer_language)\n",
    "    bm25_ef = BM25EmbeddingFunction(analyzer)\n",
    "    bm25_ef.load(model_path)\n",
    "    return bm25_ef\n",
    "\n",
    "# BGE Reranker 모델 Load\n",
    "def get_reranker_model(model_path:str = \"BAAI/bge-reranker-v2-m3\",\n",
    "                       device:str = \"cuda:0\"):\n",
    "    bge_rf = BGERerankFunction(\n",
    "        model_name=model_path,\n",
    "        device=device,\n",
    "        batch_size=32,\n",
    "    )\n",
    "    return bge_rf\n",
    "\n",
    "# 검색 result를 tsv 형태로 저장\n",
    "def save_to_tsv(result, output_path):\n",
    "    result_df = pd.DataFrame(result)\n",
    "    result_df.to_csv(output_path, sep='\\t', index=False)\n",
    "    print(\"Done!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load\n",
    "* dataset -> MSMARCO Passage Ranking <Dev> Dataset\n",
    "* testset -> MSMARCO Passage Ranking <Dev>'s 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passages :  3895239\n",
      "test_qid :  1324\n"
     ]
    }
   ],
   "source": [
    "# MSMARCO Dev dataset\n",
    "msmarco_dev = pd.read_csv(\"./data/top1000_dev.tsv\", sep='\\t', names=['qid', 'pid', 'query', 'passage'])\n",
    "\n",
    "# MSMARCO Query\n",
    "unique_query = pd.read_csv('/home/livin/rimo/llm/msmarco/notebook/unique_query.csv')\n",
    "\n",
    "# Passage 추출 (중복된 Passage는 제거)\n",
    "msmarco_dev_passages = msmarco_dev[\"passage\"].unique().tolist()\n",
    "\n",
    "# Pid 추출 (중복된 Pid는 제거)\n",
    "msmarco_dev_pids = msmarco_dev[\"pid\"].unique().tolist()\n",
    "\n",
    "# Test Set에 대한 Ground Truth\n",
    "test_qrels = pd.read_csv(\"./data/test_qrels.tsv\", sep='\\t', names=['qid', 'r', 'pid', 'l'])\n",
    "\n",
    "# Test Set에 대한 qid 추출\n",
    "test_qid = test_qrels[\"qid\"].tolist()\n",
    "\n",
    "print(\"passages : \", len(msmarco_dev_passages))\n",
    "print(\"test_qid : \", len(test_qid))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BM25\n",
    "bm25_ef = get_bm25_model()\n",
    "\n",
    "# BGE Reranker\n",
    "bge_rf = get_reranker_model(model_path=\"./models/kw_3_easy_train\", device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test set load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'docs_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m test_query_list \u001b[38;5;241m=\u001b[39m [unique_query[unique_query[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqid\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m qid_i][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m qid_i \u001b[38;5;129;01min\u001b[39;00m test_qid]\n\u001b[1;32m      5\u001b[0m query_embeddings \u001b[38;5;241m=\u001b[39m bm25_ef\u001b[38;5;241m.\u001b[39mencode_queries(test_query_list)\n\u001b[0;32m----> 6\u001b[0m cosine_similarities \u001b[38;5;241m=\u001b[39m cosine_similarity(\u001b[43mdocs_embeddings\u001b[49m, query_embeddings)\u001b[38;5;66;03m#.flatten()\u001b[39;00m\n\u001b[1;32m      7\u001b[0m top_n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'docs_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "test_query_list = [unique_query[unique_query[\"qid\"] == qid_i][\"query\"].tolist()[0] for qid_i in test_qid]\n",
    "top_n = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve\n",
    "* 1차 retriever -> Milvus/BM25\n",
    "* 2차 retriever -> BAAI/bge-reranker-v2-m3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MilvusClient()\n",
    "client.load_collection(\"msmarco_bm25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1324 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▎| 1239/1324 [05:25<00:23,  3.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "983451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1324/1324 [05:48<00:00,  3.80it/s]\n"
     ]
    }
   ],
   "source": [
    "# 24분 소요\n",
    "result = []\n",
    "\n",
    "for i in tqdm(range(len(test_query_list))):\n",
    "    try:\n",
    "        qid = test_qid[i]\n",
    "        query = test_query_list[i]\n",
    "\n",
    "        query_embeddings = bm25_ef.encode_queries([query])\n",
    "\n",
    "        res = client.search(\n",
    "            collection_name=\"msmarco_bm25\",  # target collection\n",
    "            data=query_embeddings, \n",
    "            limit=100,  # number of returned entities\n",
    "            output_fields=[\"pid\",\"text\"],  # specifies fields to be returned\n",
    "            anns_field=\"sparse_vector\",\n",
    "            )\n",
    "\n",
    "        candidate_pids = [entity[\"entity\"][\"pid\"] for entity in res[0]]\n",
    "        candidate_passages = [entity[\"entity\"][\"text\"] for entity in res[0]]\n",
    "\n",
    "        top_k = bge_rf(\n",
    "                    query=query,\n",
    "                    documents=candidate_passages,\n",
    "                    top_k=100,\n",
    "                )\n",
    "        for n,i in enumerate(top_k):\n",
    "                result.append([qid, candidate_pids[i.index], n+1])\n",
    "    except:\n",
    "        print(qid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "save_to_tsv(result, \"./result/bm25_reranker_collection.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
